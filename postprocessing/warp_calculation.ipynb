{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50691d5a-345a-4844-b52f-a622694f0d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fcb329d-a8ca-4a48-a4cb-3842635133bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.8\n"
     ]
    }
   ],
   "source": [
    "!python --version or python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1da1637d-40b8-4a75-90b0-29a01d9a4db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: astropy in /opt/conda/lib/python3.12/site-packages (7.0.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.12/site-packages (from astropy) (1.26.4)\n",
      "Requirement already satisfied: pyerfa>=2.0.1.1 in /opt/conda/lib/python3.12/site-packages (from astropy) (2.0.1.5)\n",
      "Requirement already satisfied: astropy-iers-data>=0.2025.1.31.12.41.4 in /opt/conda/lib/python3.12/site-packages (from astropy) (0.2025.3.31.0.36.18)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /opt/conda/lib/python3.12/site-packages (from astropy) (6.0.2)\n",
      "Requirement already satisfied: packaging>=22.0.0 in /opt/conda/lib/python3.12/site-packages (from astropy) (24.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install astropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1df73bc-2e2c-40be-b058-8da3f19286c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astropy import wcs\n",
    "from astropy.io import fits\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5defd01-3389-4b08-8f27-5fc14c60a137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fec20c8a-4eeb-4aa1-a535-446be01aad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_file(args) -> None:\n",
    "#     file_path, destination_path = args\n",
    "#     os.makedirs(destination_path, exist_ok=True)\n",
    "#     try:\n",
    "#         hdu = fits.open(file_path)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error opening file {file_path}: {e}\")\n",
    "#         missing_files.append(file_path)\n",
    "#         return\n",
    "\n",
    "#     # Extracting information from the header\n",
    "#     CRPIX1 = hdu[0].header[\"CRPIX1\"]\n",
    "#     CRPIX2 = hdu[0].header[\"CRPIX2\"]\n",
    "#     CRVAL1 = hdu[0].header[\"CRVAL1\"]\n",
    "#     CRVAL2 = hdu[0].header[\"CRVAL2\"]\n",
    "#     PC1_1 = hdu[0].header[\"CD1_1\"]\n",
    "#     PC1_2 = hdu[0].header[\"CD1_2\"]\n",
    "#     PC2_1 = hdu[0].header[\"CD2_1\"]\n",
    "#     PC2_2 = hdu[0].header[\"CD2_2\"]\n",
    "#     CTYPE1 = hdu[0].header[\"CTYPE1\"]\n",
    "#     CTYPE2 = hdu[0].header[\"CTYPE2\"]\n",
    "\n",
    "#     # Creating WCS object\n",
    "#     w2 = wcs.WCS(naxis=2)\n",
    "#     w2.wcs.crpix = [CRPIX1, CRPIX2]\n",
    "#     w2.wcs.pc = [[PC1_1, PC1_2], [PC2_1, PC2_2]]\n",
    "#     w2.wcs.crval = [CRVAL1, CRVAL2]\n",
    "#     w2.wcs.ctype = [CTYPE1, CTYPE2]\n",
    "\n",
    "#     # Opening a document to write the results\n",
    "#     # name = re.split('[. /]', file_path)\n",
    "#     # results = open(name[1] + '_u.txt', 'w')\n",
    "#     # results.write('X Y RA2000 DEC2000 Value\\n')\n",
    "#     #\n",
    "#     # # Looping through the data and transforming pixels\n",
    "#     # for i in range(1, len(hdu[0].data) + 1):\n",
    "#     #     for j in range(1, len(hdu[0].data[i - 1]) + 1):\n",
    "#     #         if not np.isnan(hdu[0].data[i - 1][j - 1]):\n",
    "#     #             # Correcting the indexing\n",
    "#     #             pixel_coords = np.array([[j, i]])\n",
    "#     #             radec = w2.pixel_to_world(j+1,i+1)\n",
    "#     #             #RA_of_the_pixel = radec.ra.hour\n",
    "#     #\n",
    "#     #             #DEC_of_the_pixel = radec.dec.degree\n",
    "#     #             RA_of_the_pixel = 1\n",
    "#     #\n",
    "#     #             DEC_of_the_pixel = 1\n",
    "#     #             #sky_coord = SkyCoord(ra=world_coords[0, 0], dec=world_coords[0, 1], unit=(u.deg, u.deg), frame='fk5')\n",
    "#     #             #RA_of_the_pixel = sky_coord.ra.to_string(unit=u.hour, sep=':', precision=2, pad=True)\n",
    "#     #             #DEC_of_the_pixel = sky_coord.dec.to_string(unit=u.degree, sep=':', precision=1, pad=True)\n",
    "#     #\n",
    "#     #             results.write('%d %d %s %s %.8f\\n' % (j, i, RA_of_the_pixel, DEC_of_the_pixel, hdu[0].data[i - 1][j - 1]))\n",
    "#     #\n",
    "#     # # Closing the files\n",
    "#     # hdu.close()\n",
    "#     # results.close()\n",
    "#     data = hdu[0].data\n",
    "#     y_indices, x_indices = np.indices(data.shape)\n",
    "#     valid_mask = ~np.isnan(data) & (data != 0)\n",
    "#     # Filter valid indices\n",
    "#     x_valid = x_indices[valid_mask]\n",
    "#     y_valid = y_indices[valid_mask]\n",
    "#     values_valid = data[valid_mask]\n",
    "#     # Transform pixel coordinates to world coordinates\n",
    "#     radec = w2.pixel_to_world(x_valid + 1, y_valid + 1)\n",
    "#     RA_of_the_pixel, DEC_of_the_pixel = 1, 1  # Placeholder values\n",
    "#     file_name = file_path.split('/')[-1].split('.')[0]\n",
    "#     results_filename = f\"{file_name}_u.txt\"\n",
    "#     # Write results to file\n",
    "#     with open(results_filename, 'w') as result_file:\n",
    "#         result_file.write('X Y RA2000 DEC2000 Value\\n')\n",
    "#         for x, y, value in zip(x_valid, y_valid, values_valid):\n",
    "#             result_file.write(f'{x + 1} {y + 1} {RA_of_the_pixel} {DEC_of_the_pixel} {value:.8f}\\n')\n",
    "#     hdu.close()\n",
    "#     #################################\n",
    "#     data = pd.read_csv(results_filename,sep='\\s+',na_values=[\"\"],keep_default_na=False,header=0,encoding='latin1',names=('X','Y','rad','dec','value'))\n",
    "#     os.remove(results_filename)\n",
    "#     # data=data.drop(data[(data.value==0)].index)\n",
    "#     data = data[data.value != 0]\n",
    "\n",
    "#     plt.scatter(data.X,data.value,marker='.')\n",
    "#     #plt.show()\n",
    "\n",
    "#     #skalovanie\n",
    "#     scaler = MinMaxScaler(feature_range=(-70, 70))\n",
    "#     y_arr= data.Y.values.reshape(-1, 1)\n",
    "#     data['Y']=scaler.fit_transform(y_arr)\n",
    "\n",
    "#     scaler1 = MinMaxScaler(feature_range=(-300, 300))\n",
    "#     x_arr= data.X.values.reshape(-1, 1)\n",
    "#     data['X']=scaler1.fit_transform(x_arr)\n",
    "\n",
    "#     stred_X=(data.X.max()+data.X.min())/2\n",
    "#     stred_Y=(data.Y.max()+data.Y.min())/2\n",
    "\n",
    "#     X_max1 = data['value'].idxmax()\n",
    "\n",
    "#     stred_X1=data['X'].loc[[X_max1]]\n",
    "#     stred_Y1=data['Y'].loc[[X_max1]]\n",
    "\n",
    "#     data['X']=data.X-float(stred_X)\n",
    "#     data['Y']=data.Y-float(stred_Y)\n",
    "\n",
    "#     plt.scatter(data.X,data.Y,c=data.value,label='_nolegend_', norm=colors.LogNorm())\n",
    "#     #plt.show()\n",
    "#     #data.to_csv('tri.txt',sep='\\t',index=False,header=False)\n",
    "\n",
    "#     #rotacia\n",
    "#     # Fit a linear regression model to find the central line, using a subset of data\n",
    "#     data_filtered = data[(data['X'] >= -100) & (data['X'] <= 100)]\n",
    "\n",
    "#     x_range1 = data_filtered['X'].max() - data_filtered['X'].min()\n",
    "#     y_range1 = data_filtered['Y'].max() - data_filtered['Y'].min()\n",
    "#     scale_factor = y_range1 / x_range1\n",
    "\n",
    "#     # Scale the X axis (only for regression)\n",
    "#     data_scaled = data_filtered.copy()\n",
    "#     data_scaled['X'] = data_filtered['X'] * scale_factor\n",
    "\n",
    "#     X = data_scaled['X'].values.reshape(-1, 1)\n",
    "#     Y = data_scaled['Y'].values\n",
    "#     reg = LinearRegression().fit(X, Y)\n",
    "#     slope = reg.coef_[0]\n",
    "#     intercept = reg.intercept_\n",
    "\n",
    "#     # Calculate the angle of rotation\n",
    "#     angle = np.arctan(slope)\n",
    "\n",
    "#     # Define a function to rotate points\n",
    "#     def rotate_point(x, y, angle):\n",
    "#         cos_theta = np.cos(angle)\n",
    "#         sin_theta = np.sin(angle)\n",
    "#         x_new = x * cos_theta - y * sin_theta\n",
    "#         y_new = x * sin_theta + y * cos_theta\n",
    "#         return x_new, y_new\n",
    "\n",
    "#     # Rotate the entire galaxy using the angle found\n",
    "#     data_rot = data.copy()\n",
    "#     data_rot['X_rot'], data_rot['Y_rot'] = zip(*data.apply(lambda row: rotate_point(row['X'], row['Y'], -angle), axis=1))\n",
    "\n",
    "#     # Plotting\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     plt.scatter(data['X'], data['Y'], label='Original Data', color='blue')\n",
    "#     plt.scatter(data_rot['X_rot'], data_rot['Y_rot'], label='Rotated Data', color='red')\n",
    "#     plt.xlabel('X')\n",
    "#     plt.ylabel('Y')\n",
    "#     plt.title('Central Line of the Galaxy')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     plt.gca().set_aspect('equal', adjustable='box')  # Ensure equal aspect ratio\n",
    "#     #plt.show()\n",
    "\n",
    "#     plt.scatter(data_rot.X_rot,data_rot.Y_rot)\n",
    "#     #plt.show()\n",
    "\n",
    "#     # Define integer-spaced bins for X_rot and Y_rot\n",
    "#     x_bins = np.arange(-100, 101, 1)  # X bins from -100 to 100 with step 1\n",
    "#     y_bins = np.arange(-30, 31, 1)    # Y bins from -30 to 30 with step 1\n",
    "\n",
    "#     # Assign each point to a bin (ensuring integer values are used for proper binning)\n",
    "#     data_rot['X_bin'] = np.floor(data_rot['X_rot']).astype(int)\n",
    "#     data_rot['Y_bin'] = np.floor(data_rot['Y_rot']).astype(int)\n",
    "\n",
    "#     # Group by the X_bin and Y_bin, and calculate the mean values in each bin\n",
    "#     binned_data = data_rot.groupby(['X_bin', 'Y_bin']).agg({\n",
    "#         'X_rot': 'mean',\n",
    "#         'Y_rot': 'mean',\n",
    "#         'value': 'mean'\n",
    "#     }).reset_index()\n",
    "\n",
    "#     data=data.sort_values(by=['X'])\n",
    "\n",
    "#     data_rot=data_rot.sort_values(by=['X_rot'])\n",
    "\n",
    "#     #delime na mensie a vacsie ako 0 v Xku\n",
    "#     binned_data1=binned_data\n",
    "#     binned_data=binned_data.drop(binned_data[(binned_data.X_bin>0)].index)\n",
    "#     binned_data1=binned_data1.drop(binned_data1[(binned_data1.X_bin<0)].index)\n",
    "\n",
    "#     #skalovat vsetko na rovnake Xmax a Xmin\n",
    "#     Xmax=300\n",
    "#     Xmin=-300\n",
    "\n",
    "#     factor_min=Xmin/binned_data['X_bin'].min()\n",
    "#     factor_max=Xmax/binned_data1['X_bin'].max()\n",
    "\n",
    "#     binned_data=binned_data.reset_index(drop=True)\n",
    "#     binned_data1=binned_data1.reset_index(drop=True)\n",
    "\n",
    "\n",
    "#     #integral aby sme nasli centrum\n",
    "#     binned_data=binned_data.sort_values(by=['Y_bin','X_bin'])\n",
    "#     binned_data=binned_data.reset_index(drop=True)\n",
    "#     binned_data1=binned_data1.sort_values(by=['Y_bin','X_bin'])\n",
    "#     binned_data1=binned_data1.reset_index(drop=True)\n",
    "\n",
    "#     range_x_neg=binned_data.X_bin.drop_duplicates()\n",
    "#     range_x_neg=range_x_neg.sort_values()\n",
    "#     range_x_neg=range_x_neg.reset_index(drop=True)\n",
    "\n",
    "#     range_x_pos=binned_data1.X_bin.drop_duplicates()\n",
    "#     range_x_pos=range_x_pos.sort_values()\n",
    "#     range_x_pos=range_x_pos.reset_index(drop=True)\n",
    "\n",
    "#     def integral(dat,X):\n",
    "#         dat=dat.drop(dat[(dat.X_bin!=X)].index)\n",
    "#         dat=dat.reset_index(drop=True)\n",
    "#         int1,int2=0.0,0.0\n",
    "#         vysl=list()\n",
    "#         for i in range(1,len(dat)):\n",
    "#             dy=dat.Y_bin[i]-dat.Y_bin[i-1]\n",
    "#             a=dat.value[i]*dat.Y_bin[i]*dy\n",
    "#             b=dat.value[i]*dy\n",
    "#             int1+=a\n",
    "#             int2+=b\n",
    "#         vysl.append((X,int1,int2))\n",
    "#         vyslf=pd.DataFrame(vysl,columns=['X','int1','int2'])\n",
    "#         vyslf['integ']=vyslf.int1/vyslf.int2\n",
    "#         return vyslf\n",
    "\n",
    "#     res_neg=pd.DataFrame(columns=['X','int1','int2','integ'])\n",
    "#     for i in range_x_neg:\n",
    "#         # res_neg=res_neg._append(integral(binned_data,i),sort=False)\n",
    "#         res_neg=res_neg.append(integral(binned_data,i),sort=False)\n",
    "\n",
    "\n",
    "#     res_neg=res_neg.sort_values(by=['X'])\n",
    "#     res_neg=res_neg.reset_index(drop=True)\n",
    "\n",
    "#     res_pos=pd.DataFrame(columns=['X','int1','int2','integ'])\n",
    "#     for i in range_x_pos:\n",
    "#         # res_pos=res_pos._append(integral(binned_data1,i),sort=False)\n",
    "#         res_pos=res_pos.append(integral(binned_data1,i),sort=False)\n",
    "\n",
    "#     res_pos=res_pos.sort_values(by=['X'])\n",
    "#     res_pos=res_pos.reset_index(drop=True)\n",
    "\n",
    "#     # Define the bin size\n",
    "#     bin_size = 30\n",
    "\n",
    "#     # Find the minimum and maximum X values across both res_neg and res_pos\n",
    "#     min_x_neg = res_neg['X'].min()\n",
    "#     max_x_pos = res_pos['X'].max()\n",
    "\n",
    "#     # Create bin edges ensuring continuity across zero\n",
    "#     bins = np.arange(min_x_neg, max_x_pos + bin_size, bin_size)\n",
    "\n",
    "#     # Bin the X values and aggregate\n",
    "#     res_neg['X_bin'] = pd.cut(res_neg['X'], bins=bins, include_lowest=True, right=False)\n",
    "#     res_pos['X_bin'] = pd.cut(res_pos['X'], bins=bins, include_lowest=True, right=False)\n",
    "\n",
    "#     # Aggregate the data by bins\n",
    "#     res_neg_binned = res_neg.groupby('X_bin').agg({'X': 'mean', 'int1': 'sum', 'int2': 'sum', 'integ': 'mean'}).reset_index(drop=True)\n",
    "#     res_pos_binned = res_pos.groupby('X_bin').agg({'X': 'mean', 'int1': 'sum', 'int2': 'sum', 'integ': 'mean'}).reset_index(drop=True)\n",
    "\n",
    "#     # Drop NaN values after binning and reset the index\n",
    "#     res_neg_binned = res_neg_binned.dropna().reset_index(drop=True)\n",
    "#     res_pos_binned = res_pos_binned.dropna().reset_index(drop=True)\n",
    "\n",
    "#     # Combine the binned data\n",
    "#     res_combined = pd.concat([res_neg_binned, res_pos_binned]).sort_values(by='X').reset_index(drop=True)\n",
    "\n",
    "#     # Interpolation function for 'integ' values based on the binned data\n",
    "#     interp_integ = interp1d(res_combined['X'], res_combined['integ'], fill_value=\"extrapolate\", kind='linear')\n",
    "\n",
    "#     # Add the first and last X values from the original data and interpolate their integ['mean'] values\n",
    "#     first_x_neg = min_x_neg\n",
    "#     last_x_pos = max_x_pos\n",
    "\n",
    "#     # Create a DataFrame for the interpolated values at the edges\n",
    "#     edge_data = pd.DataFrame({\n",
    "#         'X': [first_x_neg, last_x_pos],\n",
    "#         'int1': [np.nan, np.nan],  # We'll only interpolate 'integ'\n",
    "#         'int2': [np.nan, np.nan],\n",
    "#         'integ': [interp_integ(first_x_neg), interp_integ(last_x_pos)]\n",
    "#     })\n",
    "\n",
    "#     # Append the interpolated values to the combined results\n",
    "#     res_combined = pd.concat([res_combined, edge_data]).sort_values(by='X').reset_index(drop=True)\n",
    "\n",
    "#     warp_left = res_combined['integ'].iloc[0]\n",
    "#     warp_right = res_combined['integ'].iloc[-1]\n",
    "\n",
    "#     with open(f'{destination_path}/{file_name}_warp.txt', 'w') as warp_file:\n",
    "#         warp_file.write(f'min: {warp_left}\\n')\n",
    "#         warp_file.write(f'max: {warp_right}\\n')\n",
    "\n",
    "# #     plt.scatter(data_rot.X_rot, data_rot.Y_rot, c=data_rot.value, label='_nolegend_', norm=colors.LogNorm(vmin=10**-3, vmax=1), s=10)\n",
    "# #     plt.plot(res_combined.X,res_combined.integ,color='red')\n",
    "# #     plt.colorbar(label='Flux',extend='both')\n",
    "# #     plt.xlabel('$X$ [Pixels]')\n",
    "# #     plt.ylabel('$Y_{max}$ [Pixels]')\n",
    "# # #    plt.ylim(-50,50)\n",
    "# #     plt.savefig(f'{destination_path}/{file_name}_outline.pdf', format='pdf')\n",
    "# #     #plt.show()\n",
    "# #     plt.clf()\n",
    "# #     plt.close()\n",
    "#     parts = file_path.split('/')\n",
    "#     folder_name = parts[-2]  # Extracts 'folder_1'\n",
    "#     file_base = parts[-1].rsplit('.', 2)[0]  # Removes extensions like .fits.gz2\n",
    "#     image_path = f\"{folder_name}/{file_base}.jpg\"\n",
    "#     galaxy_path = f\"{GALAXY_PATH}/{image_path}\"\n",
    "#     mask_path = f\"{GALAXY_MASK_PATH}/{image_path}\"\n",
    "\n",
    "#     pdf_path = f'{destination_path}/{file_name}_outline.pdf' \n",
    "\n",
    "#     with PdfPages(pdf_path) as pdf:\n",
    "#         # Page 1: Graph\n",
    "#         fig, ax = plt.subplots()  # Adjust figure size as needed\n",
    "#         scatter = ax.scatter(data_rot.X_rot, data_rot.Y_rot, c=data_rot.value, label='_nolegend_', \n",
    "#                              norm=colors.LogNorm(vmin=10**-3, vmax=1), s=10)\n",
    "#         ax.plot(res_combined.X, res_combined.integ, color='red')\n",
    "#         cbar = plt.colorbar(scatter, ax=ax, label='Flux', extend='both')\n",
    "#         ax.set_xlabel('$X$ [Pixels]')\n",
    "#         ax.set_ylabel('$Y_{max}$ [Pixels]')\n",
    "#         pdf.savefig(fig)  # Save page 1\n",
    "#         plt.close(fig)  # Close the figure\n",
    "\n",
    "#         # Page 2: Two Images\n",
    "#         img1 = mpimg.imread(galaxy_path)\n",
    "#         img2 = mpimg.imread(mask_path)\n",
    "\n",
    "#         img1_height, img1_width = img1.shape[:2]\n",
    "#         img2_height, img2_width = img2.shape[:2]\n",
    "\n",
    "#         # Maintain aspect ratio while scaling\n",
    "#         scale_factor = 3  # Adjust this to control final image size\n",
    "#         new_width1, new_height1 = img1_width * scale_factor, img1_height * scale_factor\n",
    "#         new_width2, new_height2 = img2_width * scale_factor, img2_height * scale_factor\n",
    "\n",
    "#         fig, axes = plt.subplots(1, 2, figsize=(new_width1 / 100, new_height1 / 100))  # Scale figsize dynamically\n",
    "\n",
    "#         for ax in axes:\n",
    "#             ax.axis('off')\n",
    "\n",
    "#         axes[0].imshow(img1)\n",
    "#         axes[1].imshow(img2)\n",
    "\n",
    "#         pdf.savefig(fig)  # Save page 2\n",
    "#         plt.close(fig)  # Close the figure\n",
    "\n",
    "# def get_existing():\n",
    "#     file_list = [] \n",
    "#     for root, dirs, files in os.walk(OUTPUT_DIR): \n",
    "#         for file in files:\n",
    "#             if file.endswith(\"_outline.pdf\"):\n",
    "#                 file_list.append(file.split(\"_outline.pdf\")[0] + \".fits.bz2\")\n",
    "#     return file_list\n",
    "\n",
    "# ##### for structure with folders (folder1, folder2 -> data2/uniba )    \n",
    "# # def collect_files(input_dir):\n",
    "# #    existing_files = get_existing()\n",
    "# #    print(f\"Found {len(existing_files)} existing files\")\n",
    "# #    tasks = []\n",
    "# #    for folder in os.listdir(input_dir):\n",
    "# #        if folder == \".ipynb_checkpoints\":\n",
    "# #            continue\n",
    "# #        folder_path = os.path.join(input_dir, folder)\n",
    "# #        for file in os.listdir(folder_path):\n",
    "# #            #if file.endswith(\".fits.bz2\") and file not in existing_files:\n",
    "# #             if file.endswith(\".fit.gz\") and file not in existing_files:\n",
    "# #                file_path = os.path.join(folder_path, file)\n",
    "# #                destination_path = os.path.join(OUTPUT_DIR, folder)\n",
    "# #                tasks.append((file_path, destination_path))\n",
    "# #    return tasks\n",
    "\n",
    "# ##### for simple folder (one folder with fits)\n",
    "# def collect_files(input_dir):\n",
    "#    existing_files = get_existing()\n",
    "#    print(f\"Found {len(existing_files)} existing files\")\n",
    "#    tasks = []\n",
    "\n",
    "#    for file in os.listdir(input_dir):\n",
    "#            #if file.endswith(\".fits.bz2\") and file not in existing_files:\n",
    "#         if file.endswith(\".fit.gz\") and file not in existing_files:\n",
    "#            file_path = os.path.join(input_dir, file)\n",
    "#            tasks.append((file_path, OUTPUT_DIR))\n",
    "#    return tasks\n",
    "\n",
    "\n",
    "# ##### for loading fits from list (txt file)\n",
    "# # def collect_files(txt_file_path):\n",
    "# # tasks = []\n",
    "\n",
    "# # # Read file names from the given text file\n",
    "# # with open(txt_file_path, 'r') as file:\n",
    "# #     for line in file:\n",
    "# #         file_name = line.strip()\n",
    "# #         if file_name:\n",
    "# #             updated_name = f\"{file_name}.fits.bz2\"\n",
    "# #             file_path = f\"/home/jovyan/data2/uniba/sdss_extracted_fits/{updated_name}\"\n",
    "# #             tasks.append((file_path, OUTPUT_DIR))\n",
    "\n",
    "# # return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "810cc5b0-4fcc-4dfe-a666-28fe1a846a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "MASKS_EXTENSION = \"png\"\n",
    "\n",
    "\n",
    "def process_file(args) -> None:\n",
    "    file_path, destination_path = args\n",
    "    os.makedirs(destination_path, exist_ok=True)\n",
    "\n",
    "    hdu = fits.open(file_path)\n",
    "\n",
    "    # Extracting information from the header\n",
    "    CRPIX1 = hdu[0].header[\"CRPIX1\"]\n",
    "    CRPIX2 = hdu[0].header[\"CRPIX2\"]\n",
    "    CRVAL1 = hdu[0].header[\"CRVAL1\"]\n",
    "    CRVAL2 = hdu[0].header[\"CRVAL2\"]\n",
    "    PC1_1 = hdu[0].header[\"CD1_1\"]\n",
    "    PC1_2 = hdu[0].header[\"CD1_2\"]\n",
    "    PC2_1 = hdu[0].header[\"CD2_1\"]\n",
    "    PC2_2 = hdu[0].header[\"CD2_2\"]\n",
    "    CTYPE1 = hdu[0].header[\"CTYPE1\"]\n",
    "    CTYPE2 = hdu[0].header[\"CTYPE2\"]\n",
    "\n",
    "    # Creating WCS object\n",
    "    w2 = wcs.WCS(naxis=2)\n",
    "    w2.wcs.crpix = [CRPIX1, CRPIX2]\n",
    "    w2.wcs.pc = [[PC1_1, PC1_2], [PC2_1, PC2_2]]\n",
    "    w2.wcs.crval = [CRVAL1, CRVAL2]\n",
    "    w2.wcs.ctype = [CTYPE1, CTYPE2]\n",
    "\n",
    "    # Opening a document to write the results\n",
    "    # name = re.split('[. /]', file_path)\n",
    "    # results = open(name[1] + '_u.txt', 'w')\n",
    "    # results.write('X Y RA2000 DEC2000 Value\\n')\n",
    "    #\n",
    "    # # Looping through the data and transforming pixels\n",
    "    # for i in range(1, len(hdu[0].data) + 1):\n",
    "    #     for j in range(1, len(hdu[0].data[i - 1]) + 1):\n",
    "    #         if not np.isnan(hdu[0].data[i - 1][j - 1]):\n",
    "    #             # Correcting the indexing\n",
    "    #             pixel_coords = np.array([[j, i]])\n",
    "    #             radec = w2.pixel_to_world(j+1,i+1)\n",
    "    #             #RA_of_the_pixel = radec.ra.hour\n",
    "    #\n",
    "    #             #DEC_of_the_pixel = radec.dec.degree\n",
    "    #             RA_of_the_pixel = 1\n",
    "    #\n",
    "    #             DEC_of_the_pixel = 1\n",
    "    #             #sky_coord = SkyCoord(ra=world_coords[0, 0], dec=world_coords[0, 1], unit=(u.deg, u.deg), frame='fk5')\n",
    "    #             #RA_of_the_pixel = sky_coord.ra.to_string(unit=u.hour, sep=':', precision=2, pad=True)\n",
    "    #             #DEC_of_the_pixel = sky_coord.dec.to_string(unit=u.degree, sep=':', precision=1, pad=True)\n",
    "    #\n",
    "    #             results.write('%d %d %s %s %.8f\\n' % (j, i, RA_of_the_pixel, DEC_of_the_pixel, hdu[0].data[i - 1][j - 1]))\n",
    "    #\n",
    "    # # Closing the files\n",
    "    # hdu.close()\n",
    "    # results.close()\n",
    "    data = hdu[0].data\n",
    "    y_indices, x_indices = np.indices(data.shape)\n",
    "    valid_mask = ~np.isnan(data) & (data != 0)\n",
    "    # Filter valid indices\n",
    "    x_valid = x_indices[valid_mask]\n",
    "    y_valid = y_indices[valid_mask]\n",
    "    values_valid = data[valid_mask]\n",
    "    # Transform pixel coordinates to world coordinates\n",
    "    radec = w2.pixel_to_world(x_valid + 1, y_valid + 1)\n",
    "    RA_of_the_pixel, DEC_of_the_pixel = 1, 1  # Placeholder values\n",
    "    file_name = Path(file_path).stem.split('.')[0]\n",
    "    results_filename = f\"{file_name}_u.txt\"\n",
    "    # Write results to file\n",
    "    with open(results_filename, 'w') as result_file:\n",
    "        result_file.write('X Y RA2000 DEC2000 Value\\n')\n",
    "        for x, y, value in zip(x_valid, y_valid, values_valid):\n",
    "            result_file.write(f'{x + 1} {y + 1} {RA_of_the_pixel} {DEC_of_the_pixel} {value:.8f}\\n')\n",
    "    hdu.close()\n",
    "    #################################\n",
    "    data = pd.read_csv(results_filename,sep='\\s+',na_values=[\"\"],keep_default_na=False,header=0,encoding='latin1',names=('X','Y','rad','dec','value'))\n",
    "    os.remove(results_filename)\n",
    "    # data=data.drop(data[(data.value==0)].index)\n",
    "    data = data[data.value != 0]\n",
    "\n",
    "    plt.scatter(data.X,data.value,marker='.')\n",
    "    #plt.show()\n",
    "\n",
    "    #skalovanie\n",
    "    scaler = MinMaxScaler(feature_range=(-30, 30))\n",
    "    y_arr= data.Y.values.reshape(-1, 1)\n",
    "    data['Y']=scaler.fit_transform(y_arr)\n",
    "\n",
    "    scaler1 = MinMaxScaler(feature_range=(-50, 50))\n",
    "    x_arr= data.X.values.reshape(-1, 1)\n",
    "    data['X']=scaler1.fit_transform(x_arr)\n",
    "\n",
    "    stred_X=(data.X.max()+data.X.min())/2\n",
    "    stred_Y=(data.Y.max()+data.Y.min())/2\n",
    "\n",
    "    X_max1 = data['value'].idxmax()\n",
    "\n",
    "    stred_X1=data['X'].loc[[X_max1]]\n",
    "    stred_Y1=data['Y'].loc[[X_max1]]\n",
    "\n",
    "    data['X']=data.X-float(stred_X)\n",
    "    data['Y']=data.Y-float(stred_Y)\n",
    "\n",
    "    plt.scatter(data.X,data.Y,c=data.value,label='_nolegend_', norm=colors.LogNorm())\n",
    "    #plt.show()\n",
    "    #data.to_csv('tri.txt',sep='\\t',index=False,header=False)\n",
    "\n",
    "    #rotacia\n",
    "    # Fit a linear regression model to find the central line, using a subset of data\n",
    "    data_filtered = data[(data['X'] >= -100) & (data['X'] <= 100)]\n",
    "\n",
    "    x_range1 = data_filtered['X'].max() - data_filtered['X'].min()\n",
    "    y_range1 = data_filtered['Y'].max() - data_filtered['Y'].min()\n",
    "    scale_factor = y_range1 / x_range1\n",
    "\n",
    "    # Scale the X axis (only for regression)\n",
    "    data_scaled = data_filtered.copy()\n",
    "    data_scaled['X'] = data_filtered['X'] * scale_factor\n",
    "\n",
    "    X = data_scaled['X'].values.reshape(-1, 1)\n",
    "    Y = data_scaled['Y'].values\n",
    "    reg = LinearRegression().fit(X, Y)\n",
    "    slope = reg.coef_[0]\n",
    "    intercept = reg.intercept_\n",
    "\n",
    "    # Calculate the angle of rotation\n",
    "    angle = np.arctan(slope)\n",
    "\n",
    "    # Define a function to rotate points\n",
    "    def rotate_point(x, y, angle):\n",
    "        cos_theta = np.cos(angle)\n",
    "        sin_theta = np.sin(angle)\n",
    "        x_new = x * cos_theta - y * sin_theta\n",
    "        y_new = x * sin_theta + y * cos_theta\n",
    "        return x_new, y_new\n",
    "\n",
    "    # Rotate the entire galaxy using the angle found\n",
    "    data_rot = data.copy()\n",
    "    data_rot['X_rot'], data_rot['Y_rot'] = zip(*data.apply(lambda row: rotate_point(row['X'], row['Y'], -angle), axis=1))\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(data['X'], data['Y'], label='Original Data', color='blue')\n",
    "    plt.scatter(data_rot['X_rot'], data_rot['Y_rot'], label='Rotated Data', color='red')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('Central Line of the Galaxy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')  # Ensure equal aspect ratio\n",
    "    #plt.show()\n",
    "\n",
    "    plt.scatter(data_rot.X_rot,data_rot.Y_rot)\n",
    "    #plt.show()\n",
    "\n",
    "    # Define integer-spaced bins for X_rot and Y_rot\n",
    "    x_bins = np.arange(-100, 101, 1)  # X bins from -100 to 100 with step 1\n",
    "    y_bins = np.arange(-30, 31, 1)    # Y bins from -30 to 30 with step 1\n",
    "\n",
    "    # Assign each point to a bin (ensuring integer values are used for proper binning)\n",
    "    data_rot['X_bin'] = np.floor(data_rot['X_rot']).astype(int)\n",
    "    data_rot['Y_bin'] = np.floor(data_rot['Y_rot']).astype(int)\n",
    "\n",
    "    # Group by the X_bin and Y_bin, and calculate the mean values in each bin\n",
    "    binned_data = data_rot.groupby(['X_bin', 'Y_bin']).agg({\n",
    "        'X_rot': 'mean',\n",
    "        'Y_rot': 'mean',\n",
    "        'value': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    data=data.sort_values(by=['X'])\n",
    "\n",
    "    data_rot=data_rot.sort_values(by=['X_rot'])\n",
    "\n",
    "    #delime na mensie a vacsie ako 0 v Xku\n",
    "    binned_data1=binned_data\n",
    "    binned_data=binned_data.drop(binned_data[(binned_data.X_bin>0)].index)\n",
    "    binned_data1=binned_data1.drop(binned_data1[(binned_data1.X_bin<0)].index)\n",
    "\n",
    "    #skalovat vsetko na rovnake Xmax a Xmin\n",
    "    Xmax=300\n",
    "    Xmin=-300\n",
    "\n",
    "    factor_min=Xmin/binned_data['X_bin'].min()\n",
    "    factor_max=Xmax/binned_data1['X_bin'].max()\n",
    "\n",
    "    binned_data=binned_data.reset_index(drop=True)\n",
    "    binned_data1=binned_data1.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    #integral aby sme nasli centrum\n",
    "    binned_data=binned_data.sort_values(by=['Y_bin','X_bin'])\n",
    "    binned_data=binned_data.reset_index(drop=True)\n",
    "    binned_data1=binned_data1.sort_values(by=['Y_bin','X_bin'])\n",
    "    binned_data1=binned_data1.reset_index(drop=True)\n",
    "\n",
    "    range_x_neg=binned_data.X_bin.drop_duplicates()\n",
    "    range_x_neg=range_x_neg.sort_values()\n",
    "    range_x_neg=range_x_neg.reset_index(drop=True)\n",
    "\n",
    "    range_x_pos=binned_data1.X_bin.drop_duplicates()\n",
    "    range_x_pos=range_x_pos.sort_values()\n",
    "    range_x_pos=range_x_pos.reset_index(drop=True)\n",
    "\n",
    "    def integral(dat,X):\n",
    "        dat=dat.drop(dat[(dat.X_bin!=X)].index)\n",
    "        dat=dat.reset_index(drop=True)\n",
    "        int1,int2=0.0,0.0\n",
    "        vysl=list()\n",
    "        for i in range(1,len(dat)):\n",
    "            dy=dat.Y_bin[i]-dat.Y_bin[i-1]\n",
    "            a=dat.value[i]*dat.Y_bin[i]*dy\n",
    "            b=dat.value[i]*dy\n",
    "            int1+=a\n",
    "            int2+=b\n",
    "        vysl.append((X,int1,int2))\n",
    "        vyslf=pd.DataFrame(vysl,columns=['X','int1','int2'])\n",
    "        vyslf['integ']=vyslf.int1/vyslf.int2\n",
    "        return vyslf\n",
    "\n",
    "    res_neg=pd.DataFrame(columns=['X','int1','int2','integ'])\n",
    "    for i in range_x_neg:\n",
    "        res_neg=res_neg._append(integral(binned_data,i),sort=False)\n",
    "        #res_neg=res_neg.append(integral(binned_data,i),sort=False)\n",
    "\n",
    "\n",
    "    res_neg=res_neg.sort_values(by=['X'])\n",
    "    res_neg=res_neg.reset_index(drop=True)\n",
    "\n",
    "    res_pos=pd.DataFrame(columns=['X','int1','int2','integ'])\n",
    "    for i in range_x_pos:\n",
    "        res_pos=res_pos._append(integral(binned_data1,i),sort=False)\n",
    "        #res_pos=res_pos.append(integral(binned_data1,i),sort=False)\n",
    "\n",
    "    res_pos=res_pos.sort_values(by=['X'])\n",
    "    res_pos=res_pos.reset_index(drop=True)\n",
    "\n",
    "    # Define the bin size\n",
    "    bin_size = 5\n",
    "\n",
    "    # Find the minimum and maximum X values across both res_neg and res_pos\n",
    "    min_x_neg = res_neg['X'].min()\n",
    "    max_x_pos = res_pos['X'].max()\n",
    "\n",
    "    # Create bin edges ensuring continuity across zero\n",
    "    bins = np.arange(min_x_neg, max_x_pos + bin_size, bin_size)\n",
    "\n",
    "    # Bin the X values and aggregate\n",
    "    res_neg['X_bin'] = pd.cut(res_neg['X'], bins=bins, include_lowest=True, right=False)\n",
    "    res_pos['X_bin'] = pd.cut(res_pos['X'], bins=bins, include_lowest=True, right=False)\n",
    "\n",
    "    # Aggregate the data by bins\n",
    "    res_neg_binned = res_neg.groupby('X_bin').agg({'X': 'mean', 'int1': 'sum', 'int2': 'sum', 'integ': 'mean'}).reset_index(drop=True)\n",
    "    res_pos_binned = res_pos.groupby('X_bin').agg({'X': 'mean', 'int1': 'sum', 'int2': 'sum', 'integ': 'mean'}).reset_index(drop=True)\n",
    "\n",
    "    # Drop NaN values after binning and reset the index\n",
    "    res_neg_binned = res_neg_binned.dropna().reset_index(drop=True)\n",
    "    res_pos_binned = res_pos_binned.dropna().reset_index(drop=True)\n",
    "\n",
    "    # Combine the binned data\n",
    "    res_combined = pd.concat([res_neg_binned, res_pos_binned]).sort_values(by='X').reset_index(drop=True)\n",
    "\n",
    "    # Interpolation function for 'integ' values based on the binned data\n",
    "    interp_integ = interp1d(res_combined['X'], res_combined['integ'], fill_value=\"extrapolate\", kind='linear')\n",
    "\n",
    "    # Add the first and last X values from the original data and interpolate their integ['mean'] values\n",
    "    first_x_neg = min_x_neg\n",
    "    last_x_pos = max_x_pos\n",
    "\n",
    "    # Create a DataFrame for the interpolated values at the edges\n",
    "    edge_data = pd.DataFrame({\n",
    "        'X': [first_x_neg, last_x_pos],\n",
    "        'int1': [np.nan, np.nan],  # We'll only interpolate 'integ'\n",
    "        'int2': [np.nan, np.nan],\n",
    "        'integ': [interp_integ(first_x_neg), interp_integ(last_x_pos)]\n",
    "    })\n",
    "\n",
    "    # Append the interpolated values to the combined results\n",
    "    res_combined = pd.concat([res_combined, edge_data]).sort_values(by='X').reset_index(drop=True)\n",
    "\n",
    "    warp_left = res_combined['integ'].iloc[0]\n",
    "    warp_right = res_combined['integ'].iloc[-1]\n",
    "\n",
    "    #print('min:',warp_left)\n",
    "    #print('max:',warp_right)\n",
    "    with open(f'{destination_path}/{file_name}_warp.txt', 'w') as warp_file:\n",
    "        warp_file.write(f'min: {warp_left}\\n')\n",
    "        warp_file.write(f'max: {warp_right}\\n')\n",
    "\n",
    "    # plt.scatter(data_rot.X_rot, data_rot.Y_rot, c=data_rot.value, label='_nolegend_', norm=colors.LogNorm(vmin=10**-3, vmax=1), s=10)\n",
    "    # plt.plot(res_combined.X,res_combined.integ,color='red')\n",
    "    # plt.colorbar(label='Flux',extend='both')\n",
    "    # plt.xlabel('$X$ [Pixels]')\n",
    "    # plt.ylabel('$Y_{max}$ [Pixels]')\n",
    "    # #plt.ylim(-50,50)\n",
    "    # plt.savefig(f'{destination_path}/{file_name}_outline.pdf', format='pdf')\n",
    "    # #plt.show()\n",
    "\n",
    "    #parts = file_path.split('/')\n",
    "    #folder_name = parts[-2]  # Extracts 'folder_1'\n",
    "    file_base = Path(file_path).stem.rsplit('.', 2)[0]  # Removes extensions like .fits.gz2\n",
    "    #image_path = f\"{file_base}.png\"\n",
    "    #galaxy_path = f\"{GALAXY_PATH}/{image_path}\"\n",
    "    galaxy_path = f\"{GALAXY_PATH}/{file_base}.jpg\"\n",
    "    mask_path = f\"{GALAXY_MASK_PATH}/{file_base}.{MASKS_EXTENSION}\"\n",
    "\n",
    "    pdf_path = f'{destination_path}/{file_name}_outline_bin5.pdf'\n",
    "\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        # Page 1: Graph\n",
    "        fig, ax = plt.subplots()  # Adjust figure size as needed\n",
    "        scatter = ax.scatter(data_rot.X_rot, data_rot.Y_rot, c=data_rot.value, label='_nolegend_',\n",
    "                             norm=colors.LogNorm(), s=10)\n",
    "        ax.plot(res_combined.X, res_combined.integ, color='red')\n",
    "        cbar = plt.colorbar(scatter, ax=ax, label='Flux', extend='both')\n",
    "        ax.set_xlabel('$X$ [Pixels]')\n",
    "        ax.set_ylabel('$Y_{max}$ [Pixels]')\n",
    "        pdf.savefig(fig)  # Save page 1\n",
    "        plt.close(fig)  # Close the figure\n",
    "\n",
    "        # Page 2: Two Images\n",
    "        img1 = mpimg.imread(galaxy_path)\n",
    "        img2 = mpimg.imread(mask_path)\n",
    "\n",
    "        img1_height, img1_width = img1.shape[:2]\n",
    "        img2_height, img2_width = img2.shape[:2]\n",
    "\n",
    "        # Maintain aspect ratio while scaling\n",
    "        scale_factor = 3  # Adjust this to control final image size\n",
    "        new_width1, new_height1 = img1_width * scale_factor, img1_height * scale_factor\n",
    "        new_width2, new_height2 = img2_width * scale_factor, img2_height * scale_factor\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(new_width1 / 100, new_height1 / 100))  # Scale figsize dynamically\n",
    "\n",
    "        for ax in axes:\n",
    "            ax.axis('off')\n",
    "\n",
    "        axes[0].imshow(img1)\n",
    "        axes[1].imshow(img2)\n",
    "\n",
    "        pdf.savefig(fig)  # Save page 2\n",
    "        plt.close(fig)  # Close the figure\n",
    "\n",
    "def collect_files(input_dir):\n",
    "    tasks = []\n",
    "    # for folder in os.listdir(input_dir):\n",
    "    #     if folder == \".ipynb_checkpoints\":\n",
    "    #         continue\n",
    "    #     folder_path = os.path.join(input_dir, folder)\n",
    "    for file in os.listdir(input_dir):\n",
    "        if file.endswith(\".fits\"):\n",
    "            file_path = os.path.join(input_dir, file)\n",
    "            # destination_path = os.path.join(OUTPUT_DIR, folder)\n",
    "            destination_path = os.path.join(OUTPUT_DIR)\n",
    "            tasks.append((file_path, destination_path))\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bf2c204-aac7-4760-ad67-2c462edeb9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 files to process.\n"
     ]
    }
   ],
   "source": [
    "#INPUT_DIR = '/home/jovyan/data2/uniba/sdss_extracted_fits'\n",
    "#INPUT_DIR = '/home/jovyan/data2/uniba/file_paths.txt'\n",
    "#OUTPUT_DIR = '/home/jovyan/data2/uniba/sdss_top_galaxies_warp_calculations'\n",
    "#GALAXY_PATH = '/home/jovyan/data2/uniba/sdss_cropped_galaxies'\n",
    "#GALAXY_MASK_PATH = '/home/jovyan/data2/uniba/sdss_cropped_masks'\n",
    "\n",
    "INPUT_DIR = '/home/jovyan/data/lightning/JulianaGazdova/data/new_data_subset/rucne_masky/cut_fits_masked'\n",
    "OUTPUT_DIR = '/home/jovyan/data/lightning/JulianaGazdova/data/new_data_subset/rucne_masky/warp_calculations'\n",
    "GALAXY_PATH = '/home/jovyan/data/lightning/JulianaGazdova/data/new_data_subset/galaxies'\n",
    "GALAXY_MASK_PATH = '/home/jovyan/data/lightning/JulianaGazdova/data/new_data_subset/rucne_masky/masks_scss'\n",
    "missing_files = []\n",
    "\n",
    "tasks = collect_files(INPUT_DIR)\n",
    "print(f\"Found {len(tasks)} files to process.\")\n",
    "\n",
    "# Use multiprocessing to process files\n",
    "with Pool(processes=cpu_count()) as pool:\n",
    "    pool.map(process_file, tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e95267-bc82-42f9-9c25-1fbf85065a19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
